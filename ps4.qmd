---
title: "PSet 4"
author: "Charisma Lambert and Prashanthi Subbiah"
date: 11-02-2024
format: 
  html:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

IMPORTANT NOTE 1:
Our qmd was able to knit to show the code and output from Section 1 Q1 until Section 5 Q1. Section 5 Q2 onwards, the kernal kept crashing for on partners’ systems. Thus, we wrote to Professor Maggie Shi, Professor Peter Ganong, and TA Ozzy Houck. We were instructed to comment out the codes of Section 5. They had also mentioned that while grading, the grader should run these codes on their system, and if it runs, we would get full credit, and if the output does not fully match the solutions, we would get partial credit.  The codes for the following questions have been commented out as per these guidelines. 

IMPORTANT NOTE 2: 
With reference to the following response from Professor Maggie Shi to our Ed Post, we have reduced the number of times we have read the file in our latest qmd so that running the code takes up less memory:

"Ozzy took a look at your submission this morning and had the following comment: I tried to run Charisma and Prashanthi's code and got an error in the sections they were having trouble in. I think their issue was partially that they kept re-reading in the data under different names and so I'm guessing the kernel was crashing because they ran out of memory.

If you can fix the code only to improve memory usage and can get it to knit, you can resubmit. Otherwise the graders will just grade based on your original submission. I'll have @Ozzy  re-open the submission for you, which will close at 11:59."

Our kernels are still crashing when we run Section 5, but we are hoping that this updated code has a better chance at running on your system. We have replied in a post on Ed discussion on the initial thread. Thanks for the help!

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person Partner 1.
• Partner 1 (name and cnet ID):Charisma Lambert, charisml
• Partner 2 (name and cnet ID): Prashanthi Subbiah, prashanthis
3. Partner 1 will accept the ps4 and then share the link it creates with their partner.
You can only share it with one partner so you will not be able to change it after your
partner has accepted.
4. “This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: **CL****PS**.
5. “I have uploaded the names of anyone else other than my partner and I worked with
on the problem set here” (Ahona Roy) (1 point)
6. Late coins used this pset: **1** Late coins left after submission: **3**
7. Knit your ps4.qmd to an PDF file to make ps4.pdf,
• The PDF should not be more than 25 pages. Use head() and re-size figures when
appropriate.
8. (Partner 1): push ps4.qmd and ps4.pdf to your github repo.
9. (Partner 1): submit ps4.pdf via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

# Section 1: Download and explore the Provider of Services (POS) file (10 pts) Partner 1

1. 
```{python}
  import pandas as pd
  import os
  import csv
  import warnings
  warnings.filterwarnings("ignore")

  base_path = r"/Users/charismalambert/Downloads"
  
  health_path_16 = os.path.join(base_path, "pos2016.csv")
  health_data_16 = pd.read_csv(health_path_16)
```

I pulled the following variables:  
Provider code: PRVDR_CTGRY_CD and PRVDR_CTGRY_SBTYP_CD
CMS certification number: PRVDR_NUM
Termination code: PGM_TRMNTN_CD
Facility Name: FAC_NAME
Zipcode: ZIP_CD

2. 
```{python}
  short_term_16 = health_data_16[(health_data_16["PRVDR_CTGRY_SBTYP_CD"] == 1) & (health_data_16["PRVDR_CTGRY_CD"] == 1)]
  short_term_16["year"] = 2016

  short_term_len_16 = len(short_term_16)
  print(f"There are {short_term_len_16} hospitals reported in the 2016 data.")
```

a. There are 7,245 hospitals reported in the 2016 data.
b. I found a report from the American Hospital Association that there were 5,534 hospitals registered in the US in 2016. I think it differs because their data does not contain outliers or fuzz, such as a if a hospital closed at any point in 2016 they likely removed it from their dataset, whereas our dataset might have it for the full year. 

3. 
```{python}
  # Repeat 3 steps for 2017- 2019: 1) load data, 2) filter for short-term, and 3) find number of hospitals for that year.
  health_path_17 = os.path.join(base_path, "pos2017.csv")
  health_data_17 = pd.read_csv(health_path_17)
  short_term_17 =  health_data_17[(health_data_17["PRVDR_CTGRY_SBTYP_CD"] == 1) & (health_data_17["PRVDR_CTGRY_CD"] == 1)]
  short_term_17["year"] = 2017

  health_path_18 = os.path.join(base_path, "pos2018.csv")
  health_data_18 = pd.read_csv(health_path_18, encoding='latin1')
  short_term_18 = health_data_18[(health_data_18["PRVDR_CTGRY_SBTYP_CD"] == 1) & (health_data_18["PRVDR_CTGRY_CD"] == 1)]
  short_term_18["year"] = 2018

  health_path_19 = os.path.join(base_path, "pos2019.csv")
  health_data_19 = pd.read_csv(health_path_19, encoding='latin1')
  short_term_19 = health_data_19[(health_data_19["PRVDR_CTGRY_SBTYP_CD"] == 1) & (health_data_19["PRVDR_CTGRY_CD"] == 1)]
  short_term_19["year"] = 2019

  short_term_len_17 = len(short_term_17)
  short_term_len_18 = len(short_term_18)
  short_term_len_19 = len(short_term_19)
```

```{python}
# Append the hospital data from 2016 - 2019 together
combined_df_final = pd.concat([short_term_16, short_term_17, short_term_18, short_term_19], ignore_index= True)
combined_df_final
```

```{python}
  # Plot the number of observations by year
  observations_by_year = combined_df_final.groupby("year").size().reset_index(name = "observations")

  import altair as alt
  obs_by_year = alt.Chart(observations_by_year).mark_bar().encode(
    x = alt.X("year:O", title = "Year"),
    y = alt.Y("observations:Q", title = "Number of Hospitals", scale = alt.Scale(domain = [6000, 7400], clamp = True), axis = alt.Axis(tickMinStep = 100))
    ).properties(
    title = "Number of Short-Term Hospitals by Year") 

  obs_by_year
```


4. a.
```{python}
  # Plot the number of unique hospitals 
  unique_hospitals_yr = combined_df_final.groupby("year")["PRVDR_NUM"].nunique().reset_index()
  unique_hospitals_yr.columns = ["year", "unique_hospitals"]

  unique_hospitals_chart = alt.Chart(unique_hospitals_yr).mark_bar().encode(
    x = alt.X("year:O", title = "Year"),
    y = alt.Y("unique_hospitals:Q", title = "Number of Unique Hopsitals")).properties(title = "Number of Unique Hospitals Over the Years")
  unique_hospitals_chart
```

b. Comparing the two graphs, I am seeing that the data is pretty consistent over the years-- that there is an increase over the years. There is long-term stability of hospitals, with a slight increase from year to year, so there are more unique hospitals (new or mergers) but less hospitals than the year total.

# Section 2: Identify hospital closures in POS file (15 pts) (*) Partner 2

## Q1 
```{python}
combined_df_final['ZIP_CD'] = combined_df_final['ZIP_CD'].astype(str)

# Creating dataframe for active hospitals in 2016
certified_2016 = combined_df_final[(combined_df_final["PGM_TRMNTN_CD"] == 00) & (combined_df_final["year"] == 2016)]

# Creating dataframe for active hospitals in 2017
certified_2017 = combined_df_final[(combined_df_final["PGM_TRMNTN_CD"] == 00) & (combined_df_final["year"] == 2017)]

# Creating dataframe for active hospitals in 2018
certified_2018 = combined_df_final[(combined_df_final["PGM_TRMNTN_CD"] == 00) & (combined_df_final["year"] == 2018)]

# Creating dataframe for active hospitals in 2019
certified_2019 = combined_df_final[(combined_df_final["PGM_TRMNTN_CD"] == 00) & (combined_df_final["year"] == 2019)]

# Finding out which hospitals were closed by 2019 that were active in 2016, by PRVDR_NUM
hospitals_closed = pd.merge(certified_2016, certified_2019, on='PRVDR_NUM', how='left', indicator=True)
hospitals_closed = hospitals_closed[hospitals_closed['_merge'] == 'left_only'].drop(columns=['_merge'])

# Filtering combined_df_final for entries of hospitals in hospitals_closed (by PGM_TRMNTN_CD), when status became not active, and the year this change happened
filtered_hospitals_closed = combined_df_final[(combined_df_final['PGM_TRMNTN_CD'] != 00) & 
                            (combined_df_final['PRVDR_NUM'].isin(hospitals_closed['PRVDR_NUM']))]

# Grouping filtered_hospitals_closed by name of hospital and summarizing year of termination/disappearance
last_active_years = (filtered_hospitals_closed.groupby('FAC_NAME')
                     .agg(year_terminated_disappear =('year', 'min'), 
                          zip=('ZIP_CD', lambda x: x.unique()[0]))
                     .reset_index())

# Print how many hospitals fit the description:
print("The number of hospitals active in 2016 that are suspected to have closed by 2019 is", len(last_active_years["FAC_NAME"].unique()))
```

## Q2
```{python}
# Sorting by hospital name and printing first 10 hospitals
last_active_years.sort_values(by='FAC_NAME', ascending=True)
last_active_years_1 = last_active_years[["FAC_NAME", "year_terminated_disappear"]]
first_10 = last_active_years.head(10)
print(first_10)
```

## Q3a
```{python}
# Grouping by ZIP_CD and year, and summarizing number of active hospitals by filtering for ['PGM_TRMNTN_CD'] == 00
active_hospitals_per_year = (combined_df_final[combined_df_final['PGM_TRMNTN_CD'] == 00]
                             .groupby(['ZIP_CD', 'year']).size().reset_index(name='active_count')).reset_index()

active_hospitals_per_year = active_hospitals_per_year[active_hospitals_per_year["ZIP_CD"].isin(hospitals_closed['ZIP_CD_x'])]

# Created pivot table with columns for ZIP_CD, 2016, 2017, 2018, 2019, each summarizing number of active hospitals
pivoted_df = active_hospitals_per_year.pivot(index='ZIP_CD', columns='year', values='active_count').reset_index()

# To view entire dataframes
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', None)

# Filling 0 for NAs
pivoted_df = pivoted_df.fillna(0)

# Dataframe shows zipcodes that saw either an increase or steady number
increased_hospitals = pivoted_df[(pivoted_df[2017] >= pivoted_df[2016]) | 
                                 (pivoted_df[2018] >= pivoted_df[2017]) |
                                 (pivoted_df[2019] >= pivoted_df[2018])]

# List of unique ZIP_CD in above dataframe
no_decrease_zips = increased_hospitals['ZIP_CD'].unique()

# Merges no_decrease_zips with last_active_years to filter out (exclude) zipcodes that saw increase or no change during year of termination/disappearance/closure (final df is filtered_decreases_zips_df)
merged_df = last_active_years.merge(pivoted_df, how='left', left_on='zip', right_on='ZIP_CD')
filtered_decreases_zips_df = merged_df[~((merged_df['year_terminated_disappear'] == 2016) & (merged_df[2017] >= merged_df[2016]) |
                          (merged_df['year_terminated_disappear'] == 2017) & (merged_df[2018] >= merged_df[2017]) |
                          (merged_df['year_terminated_disappear'] == 2018) & (merged_df[2019] >= merged_df[2018]))]

# Filtering master dataframe (combined_df_final) for only those hospitals that are in filtered_decreases_zips_df
merg_aq = combined_df_final[(combined_df_final['FAC_NAME'].isin(filtered_decreases_zips_df['FAC_NAME']))]

# Dataframe with total number of hospitals in the corrected list of hospitals 
provider_count = merg_aq.groupby('FAC_NAME') \
                        .agg(NUM_CMS=('PRVDR_NUM', 'count')) \
                        .reset_index()

# Dataframe with total number of hospitals that could be mergers of acquisitions
provider_count_1 = provider_count[(provider_count['NUM_CMS'] > 1)]

print("The number of hospitals that went through mergers or aquisitions is (Answer to 3a)", len(provider_count_1))

# Used BingChat with the following query "how do I create a pivot table that has a column for ZIP_CD and each year?"
# Used BingChat with the following query "how do I check that the increase/no change in active hospitals coincides with year of termination in that zipcode"
```

## Q3b
```{python}
# Dataframe with total number of hospitals that could be mergers of acquisitions
provider_count_1 = provider_count[(provider_count['NUM_CMS'] > 1)]

### b
print("The number of hospitals in the corrected list is (Answer to 3b)", len(provider_count))
```

## Q3c
```{python}
### c
provider_count.sort_values(by='FAC_NAME', ascending=True)
print("The answer to 3c is:")
print(provider_count.head(10))

# Used BingChat with the following query "how do I create a pivot table that has a column for ZIP_CD and each year?"
# Used BingChat with the following query "how do I check that the increase/no change in active hospitals coincides with year of termination in that zipcode"
```

# Section 3: Download Census zip code shapefile (10 pt)  Partner 1
1. 
```{python}
import zipfile

zip_file_path = "/Users/charismalambert/Downloads/gz_2010_us_860_00_500k.zip"
extraction_path = "extracted_files"

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extraction_path)

files = os.listdir(extraction_path)

file_info = []

for file in files:
    file_path = os.path.join(extraction_path, file)
    file_size = os.path.getsize(file_path)  
    file_type = os.path.splitext(file)[1] 
    
    file_info.append({
        "file_name": file,
        "file_type": file_type,
        "file_size_kb": file_size / 1024 
    })

print("Answer to part b:")
for info in file_info:
    print(f" File: {info['file_name']}, Type: {info['file_type']}, Size: {info['file_size_kb']:.2f} KB")

# Citation:  Ran ChatGPT query on how to extract files from a zip file using Python and these are the steps it gave me. 
```

    a. The five file types are: 
    .xml: Metadata file describing the dataset.
    .shx: An index file that provides quick access to the shapes within the .shp file.
    .shp: Contains geographic shapes representing spatial data.
    .prj: Contains coordinate system and projection information.
    .dbf: Stores tabular attribute data associated with each spatial feature inn the .shp file.

    b. File: gz_2010_us_860_00_500k.prj, Type: .prj, Size: 0.16 KB
    File: gz_2010_us_860_00_500k.shx, Type: .shx, Size: 258.85 KB
    File: gz_2010_us_860_00_500k.shp, Type: .shp, Size: 817914.63 KB
    File: gz_2010_us_860_00_500k.dbf, Type: .dbf, Size: 6274.88 KB
    File: gz_2010_us_860_00_500k.xml, Type: .xml, Size: 15.27 KB

2. 
```{python}
# load zipcode shapefile
import geopandas as gdp

filepath = "/Users/charismalambert/Downloads/gz_2010_us_860_00_500k"
census_shp = gdp.read_file(filepath)

# restrict to Texas zip codes
census_shp["ZCTA5"] = census_shp["ZCTA5"].astype(str)
texas_zip = census_shp[census_shp["ZCTA5"].str.startswith(("75", "76", "77", "78", "79"))]

short_term_16["ZIP_CD"] = short_term_16["ZIP_CD"].astype(str)
hospitals_by_zip = short_term_16["ZIP_CD"].value_counts().reset_index()
hospitals_by_zip.columns = ["zip_code", "total_hospitals"]
hospitals_by_zipTX = texas_zip.merge(hospitals_by_zip, left_on = "ZCTA5", right_on = "zip_code", how = "left")

# choropleth of hospitals by zp code in Texas
hospitals_by_zipTX = hospitals_by_zipTX.to_crs("EPSG:5070")
hospitals_by_zipTX["area_km2"] = hospitals_by_zipTX.area/1000000
hospitals_by_zipTX.plot(column = "area_km2", legend = True).set_axis_off()

#Citation: Ran ChatGPT query on .prj file to get the .to_crs conversion. 
```

