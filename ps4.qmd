---
title: "Your Title"
author: "Charism Lambert (charisml) and Prashanthi Subbiah ()"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person Partner 1.
• Partner 1 (name and cnet ID): Charisma Lambert, charisml
• Partner 2 (name and cnet ID): Prashanthi Subbiah, prashanthis
3. Partner 1 will accept the ps4 and then share the link it creates with their partner.
You can only share it with one partner so you will not be able to change it after your
partner has accepted.
4. “This submission is our work alone and complies with the 30538 integrity policy.” Add your initials to indicate your agreement: **CL****PS**.
5. “I have uploaded the names of anyone else other than my partner and I worked with on the problem set here” (Ahona Roy) (1 point)
6. Late coins used this pset: **1** Late coins left after submission: **3**
7. Knit your ps4.qmd to an PDF file to make ps4.pdf.
The PDF should not be more than 25 pages. Use head() and re-size figures when
appropriate.
8. (Partner 1): push ps4.qmd and ps4.pdf to your github repo.
9. (Partner 1): submit ps4.pdf via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

## Style Points (10 pts)

## Submission Steps (10 pts)

## Download and explore the Provider of Services (POS) file (10 pts) Partner 1

1. 
```{python}
  import pandas as pd
  import os
  import csv

  base_path = r"/Users/charismalambert/Downloads"
  
  health_path_16 = os.path.join(base_path, "pos2016.csv")
  health_data_16 = pd.read_csv(health_path_16)
```

I pulled the following variables:  
Provider code: PRVDR_CTGRY_CD and PRVDR_CTGRY_SBTYP_CD
CMS certification number: PRVDR_NUM
Termination code: PGM_TRMNTN_CD
Facility Name: FAC_NAME
Zipcode: ZIP_CD

2. 
```{python}
  short_term_16 = health_data_16[(health_data_16["PRVDR_CTGRY_SBTYP_CD"] == 1) & (health_data_16["PRVDR_CTGRY_CD"] == 1)]
  short_term_16["year"] = 2016

  short_term_len_16 = len(short_term_16)
  print(f"There are {short_term_len_16} hospitals reported in the 2016 data.")
```

a. There are 7,245 hospitals reported in the 2016 data.
b. I found a report from the American Hospital Association that there were 5,534 hospitals registered in the US in 2016. I think it differs because their data does not contain outliers or fuzz, such as a if a hospital closed at any point in 2016 they likely removed it from their dataset, whereas our dataset might have it for the full year. 

3. 
```{python}
  # Repeat 3 steps for 2017- 2019: 1) load data, 2) filter for short-term, and 3) find number of hospitals for that year.
  health_path_17 = os.path.join(base_path, "pos2017.csv")
  health_data_17 = pd.read_csv(health_path_17)
  short_term_17 =  health_data_17[(health_data_17["PRVDR_CTGRY_SBTYP_CD"] == 1) & (health_data_17["PRVDR_CTGRY_CD"] == 1)]
  short_term_17["year"] = 2017

  health_path_18 = os.path.join(base_path, "pos2018.csv")
  health_data_18 = pd.read_csv(health_path_18, encoding='latin1')
  short_term_18 = health_data_18[(health_data_18["PRVDR_CTGRY_SBTYP_CD"] == 1) & (health_data_18["PRVDR_CTGRY_CD"] == 1)]
  short_term_18["year"] = 2018

  health_path_19 = os.path.join(base_path, "pos2019.csv")
  health_data_19 = pd.read_csv(health_path, encoding='latin1')
  short_term_19 = health_data_19[(health_data_19["PRVDR_CTGRY_SBTYP_CD"] == 1) & (health_data_19["PRVDR_CTGRY_CD"] == 1)]
  short_term_19["year"] = 2019

  short_term_len_17 = len(short_term_17)
  short_term_len_18 = len(short_term_18)
  short_term_len_19 = len(short_term_19)
```

```{python}
# Append the hospital data from 2016 - 2019 together
combined_df_final = pd.concat([short_term_16, short_term_17, short_term_18, short_term_19], ignore_index= True)
combined_df_final
```

```{python}
  # Plot the number of observations by year
  observations_by_year = combined_df_final.groupby("year").size().reset_index(name = "observations")

  import altair as alt
  obs_by_year = alt.Chart(observations_by_year).mark_bar().encode(
    x = alt.X("year:O", title = "Year"),
    y = alt.Y("observations:Q", title = "Number of Hospitals", scale = alt.Scale(domain = [6000, 7400], clamp = True), axis = alt.Axis(tickMinStep = 100))
    ).properties(
    title = "Number of Short-Term Hospitals by Year") 

  obs_by_year
```


4. a.
```{python}
  # Plot the number of unique hospitals 
  unique_hospitals_yr = combined_df_final.groupby("year")["PRVDR_NUM"].nunique().reset_index()
  unique_hospitals_yr.columns = ["year", "unique_hospitals"]

  unique_hospitals_chart = alt.Chart(unique_hospitals_yr).mark_bar().encode(
    x = alt.X("year:O", title = "Year"),
    y = alt.Y("unique_hospitals:Q", title = "Number of Unique Hopsitals")).properties(title = "Number of Unique Hospitals Over the Years")
  unique_hospitals_chart
```

b. Comparing the two graphs, I am seeing that the data is pretty consistent over the years and there is long-term stability of hospitals, with a slight dip in 2019.

## Identify hospital closures in POS file (15 pts) (*) Partner 2

1. 
2. 
3. 
    a.
    b.
    c.

## Download Census zip code shapefile (10 pt)  Partner 1

1. 
```{python}
import geopandas as gdp

zip_filepath = "/Users/charismalambert/Downloads/gz_2010_us_860_00_500k.zip"
zip_file = gdp.read_file(zip_filepath)
print(zip_file.info())
```

    a. The five file types are .xml, .shx, .shp, .prj, .dbf. 
    b. The file sizes are as follows: 
    .xml = 16KB
    .shx = 837.5 MB
    .shp = 837.5 MB
    .prj = 165 bytes
    .dbf 6.4 MB
2. 
```{python}
# load zipcode shapefile
import geopandas as gdp

filepath = "/Users/charismalambert/Downloads/gz_2010_us_860_00_500k"
census_shp = gdp.read_file(filepath)

# restrict to Texas zip codes
census_shp["ZCTA5"] = census_shp["ZCTA5"].astype(str)
texas_zip = census_shp[census_shp["ZCTA5"].str.startswith(("75", "76", "77", "78", "79"))]

short_term_16["ZIP_CD"] = short_term_16["ZIP_CD"].astype(str)
hospitals_by_zip = short_term_16["ZIP_CD"].value_counts().reset_index()
hospitals_by_zip.columns = ["zip_code", "total_hospitals"]
hospitals_by_zipTX = texas_zip.merge(hospitals_by_zip, left_on = "ZCTA5", right_on = "zip_code", how = "left")

# cloropleth of hospitals by zp code in Texas
hospitals_by_zipTX = hospitals_by_zipTX.to_crs("EPSG:5070")
hospitals_by_zipTX["area_km2"] = hospitals_by_zipTX.area/1000000
hospitals_by_zipTX.plot(column = "area_km2", legend = True).set_axis_off()
```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*) Partner 2

1. 
```{python}
from shapely.geometry import Point, Polygon
from shapely.ops import nearest_points
import time

zips_all_centroids = census_shp

# Filter Texas Border States zip codes 
zips_texas_borderstates_centroids = zips_all_centroids[ (zips_all_centroids['GEO_ID'] >= "8600000US0750") & (zips_all_centroids['GEO_ID'] < "8600000US0800") | (zips_all_centroids['GEO_ID'] == "8600000US0733") | (zips_all_centroids['GEO_ID'] == "8600000US0718") | (zips_all_centroids['GEO_ID'] == "8600000US0870") | (zips_all_centroids['GEO_ID'] == "8600000US0871") | (zips_all_centroids['GEO_ID'] >= "8600000US0873") & (zips_all_centroids['GEO_ID'] <= "8600000US0875") | (zips_all_centroids['GEO_ID'] >= "8600000US0877") & (zips_all_centroids['GEO_ID'] <= "8600000US0885") | (zips_all_centroids['GEO_ID'] == "8600000US0730") | (zips_all_centroids['GEO_ID'] == "8600000US0731") | (zips_all_centroids['GEO_ID'] >= "8600000US0734") & (zips_all_centroids['GEO_ID'] <= "8600000US0741") | (zips_all_centroids['GEO_ID'] >= "8600000US0743") & (zips_all_centroids['GEO_ID'] <= "8600000US0749") | (zips_all_centroids['GEO_ID'] >= "8600000US0716") & (zips_all_centroids['GEO_ID'] <= "8600000US0729") | (zips_all_centroids['GEO_ID'] >= "8600000US0700") & (zips_all_centroids['GEO_ID'] <= "8600000US0701") | (zips_all_centroids['GEO_ID'] >= "8600000US0703") & (zips_all_centroids['GEO_ID'] <= "8600000US0708") | (zips_all_centroids['GEO_ID'] >= "8600000US0710") & (zips_all_centroids['GEO_ID'] <= "8600000US0714") ] 

# Filter Texas zip codes 
zips_texas_centroids = zips_all_centroids[ (zips_all_centroids['GEO_ID'] >= "8600000US0750") & (zips_all_centroids['GEO_ID'] < "8600000US0800") | (zips_all_centroids['GEO_ID'] == "8600000US0733") | (zips_all_centroids['GEO_ID'] == "8600000US0718") | (zips_all_centroids['GEO_ID'] == "8600000US0885") ] 

# Adding leading 0's to combined_df_final['ZIP_CD']
combined_df_final['ZIP_CD'] = combined_df_final['ZIP_CD'].str.zfill(6)

# Dropping last digit from combined_df_final['ZIP_CD']
combined_df_final['ZIP_CD'] = combined_df_final['ZIP_CD'].str[:-1]

# Creating dataframe that has only the zip codes in Texas and Bordering hospitals that each have at least one hospital
zips_tb_with_hosp = combined_df_final[
    combined_df_final['ZIP_CD'].isin(zips_texas_borderstates_centroids['ZCTA5']) & 
    (combined_df_final["year"] == 2016)]

zips_tb_with_hosp = zips_tb_with_hosp.groupby('ZIP_CD').agg(number_of_hospitals =('PRVDR_NUM', 'count')).reset_index()
zips_tb_with_hosp = zips_tb_with_hosp[zips_tb_with_hosp['number_of_hospitals'] >= 1]
zips_tb_with_hosp
zips_tb_with_hosp_cent = zips_texas_borderstates_centroids[zips_texas_borderstates_centroids['ZCTA5'].isin(zips_tb_with_hosp['ZIP_CD'])]

# Creating ZCTA5 in combined_df_final, which has all hospital data
combined_df_final['ZCTA5'] = combined_df_final['ZIP_CD']
combined_df_final_1 = combined_df_final

## Used an inner merge on ZCTA5
zips_withhospital_centroids = pd.merge(combined_df_final_1, zips_tb_with_hosp_cent, how='inner', on='ZCTA5')
zips_withhospital_centroids

# Making sure all are geodataframes
if not isinstance(zips_withhospital_centroids, gpd.GeoDataFrame):
    zips_withhospital_centroids = gpd.GeoDataFrame(zips_withhospital_centroids, geometry='geometry')

if 'geometry' not in zips_withhospital_centroids.columns:
    zips_withhospital_centroids['geometry'] = zips_withhospital_centroids.apply(
        lambda row: Point(row['longitude'], row['latitude']), axis=1)

# Ensure they use the same CRS
zips_texas_centroids = zips_texas_centroids.to_crs(epsg=4326)
zips_withhospital_centroids = gpd.GeoDataFrame(zips_withhospital_centroids, geometry='geometry')
zips_withhospital_centroids = zips_withhospital_centroids.to_crs(epsg=4326)

# Subset to 10 ZIP codes for testing
subset = zips_texas_centroids.head(10)
subset

# Calculating time for the join with the subset
def calculate_nearest(row, other_gdf, geom_col='geometry', src_col='ZCTA5'):
    if other_gdf.empty or row[geom_col] is None:
        return None, float('inf')
    other_geom_union = other_gdf.geometry.unary_union
    nearest_geom = nearest_points(row[geom_col], other_geom_union)[1]
    nearest_point = other_gdf.loc[other_gdf.geometry == nearest_geom]
    if nearest_point.empty:
        return None, float('inf')
    nearest_zip = nearest_point[src_col].values[0]
    distance = row[geom_col].distance(nearest_geom)
    return nearest_zip, distance

start_time = time.time()

subset['nearest_zip'], subset['distance_to_nearest'] = zip(
    *subset.apply(calculate_nearest, other_gdf=zips_withhospital_centroids, axis=1)
)

end_time = time.time()
time_taken = end_time - start_time

print(subset[['ZCTA5', 'nearest_zip', 'distance_to_nearest']])
print(f"Time taken for subset of 10 ZIP codes: {time_taken} seconds")

total_zip_codes = len(zips_texas_centroids)
estimated_time = (time_taken / 10) * total_zip_codes
print(f"Estimated time for the entire dataset: {estimated_time} seconds")

# Used BingChat with the following query: "how do I create a function that calculates the calculate the distance to the nearest zip code with at least one hospital?"
# Used BingChat with the following query: "how do I measure the time for a spatial join?"

```

2. 
3. 
4. 
    a.
    b.
    c.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts) Partner 1

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
