---
title: "Your Title"
author: "Charism Lambert (charisml) and Prashanthi Subbiah ()"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person Partner 1.
• Partner 1 (name and cnet ID): Charisma Lambert, charisml
• Partner 2 (name and cnet ID): Prashanthi Subbiah, prashanthis
3. Partner 1 will accept the ps4 and then share the link it creates with their partner.
You can only share it with one partner so you will not be able to change it after your
partner has accepted.
4. “This submission is our work alone and complies with the 30538 integrity policy.” Add your initials to indicate your agreement: **CL****PS**.
5. “I have uploaded the names of anyone else other than my partner and I worked with on the problem set here” (Ahona Roy) (1 point)
6. Late coins used this pset: **1** Late coins left after submission: **3**
7. Knit your ps4.qmd to an PDF file to make ps4.pdf.
The PDF should not be more than 25 pages. Use head() and re-size figures when
appropriate.
8. (Partner 1): push ps4.qmd and ps4.pdf to your github repo.
9. (Partner 1): submit ps4.pdf via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

## Style Points (10 pts)

## Submission Steps (10 pts)

## Download and explore the Provider of Services (POS) file (10 pts) Partner 1

1. 
```{python}
  import pandas as pd
  import os
  import csv

  base_path = r"/Users/charismalambert/Downloads"
  
  health_path_16 = os.path.join(base_path, "pos2016.csv")
  health_data_16 = pd.read_csv(health_path_16)
```

I pulled the following variables:  
Provider code: PRVDR_CTGRY_CD and PRVDR_CTGRY_SBTYP_CD
CMS certification number: PRVDR_NUM
Termination code: PGM_TRMNTN_CD
Facility Name: FAC_NAME
Zipcode: ZIP_CD

2. 
```{python}
  short_term_16 = health_data_16[(health_data_16["PRVDR_CTGRY_SBTYP_CD"] == 1) & (health_data_16["PRVDR_CTGRY_CD"] == 1)]
  short_term_16["year"] = 2016

  short_term_len_16 = len(short_term_16)
  print(f"There are {short_term_len_16} hospitals reported in the 2016 data.")
```

a. There are 7,245 hospitals reported in the 2016 data.
b. I found a report from the American Hospital Association that there were 5,534 hospitals registered in the US in 2016. I think it differs because their data does not contain outliers or fuzz, such as a if a hospital closed at any point in 2016 they likely removed it from their dataset, whereas our dataset might have it for the full year. 

3. 
```{python}
  # Repeat 3 steps for 2017- 2019: 1) load data, 2) filter for short-term, and 3) find number of hospitals for that year.
  health_path_17 = os.path.join(base_path, "pos2017.csv")
  health_data_17 = pd.read_csv(health_path_17)
  short_term_17 =  health_data_17[(health_data_17["PRVDR_CTGRY_SBTYP_CD"] == 1) & (health_data_17["PRVDR_CTGRY_CD"] == 1)]
  short_term_17["year"] = 2017

  health_path_18 = os.path.join(base_path, "pos2018.csv")
  health_data_18 = pd.read_csv(health_path_18, encoding='latin1')
  short_term_18 = health_data_18[(health_data_18["PRVDR_CTGRY_SBTYP_CD"] == 1) & (health_data_18["PRVDR_CTGRY_CD"] == 1)]
  short_term_18["year"] = 2018

  health_path_19 = os.path.join(base_path, "pos2019.csv")
  health_data_19 = pd.read_csv(health_path, encoding='latin1')
  short_term_19 = health_data_19[(health_data_19["PRVDR_CTGRY_SBTYP_CD"] == 1) & (health_data_19["PRVDR_CTGRY_CD"] == 1)]
  short_term_19["year"] = 2019

  short_term_len_17 = len(short_term_17)
  short_term_len_18 = len(short_term_18)
  short_term_len_19 = len(short_term_19)
```

```{python}
# Append the hospital data from 2016 - 2019 together
short_term_over_yrs = pd.concat([short_term_16, short_term_17, short_term_18, short_term_19], ignore_index= True)
```

```{python}
  # Plot the number of observations by year
  observations_by_year = short_term_over_yrs.groupby("year").size().reset_index(name = "observations")

  import altair as alt
  obs_by_year = alt.Chart(observations_by_year).mark_bar().encode(
    x = alt.X("year:O", title = "Year"),
    y = alt.Y("observations:Q", title = "Number of Hospitals", scale = alt.Scale(domain = [6000, 7400], clamp = True), axis = alt.Axis(tickMinStep = 100))
    ).properties(
    title = "Number of Short-Term Hospitals by Year") 

  obs_by_year
```


4. a.
```{python}
  # Plot the number of unique hospitals 
  unique_hospitals_yr = short_term_over_yrs.groupby("year")["PRVDR_NUM"].nunique().reset_index()
  unique_hospitals_yr.columns = ["year", "unique_hospitals"]

  unique_hospitals_chart = alt.Chart(unique_hospitals_yr).mark_bar().encode(
    x = alt.X("year:O", title = "Year"),
    y = alt.Y("unique_hospitals:Q", title = "Number of Unique Hopsitals")).properties(title = "Number of Unique Hospitals Over the Years")
  unique_hospitals_chart
```

b. Comparing the two graphs, I am seeing that the data is pretty consistent over the years and there is long-term stability of hospitals, with a slight dip in 2019.

## Identify hospital closures in POS file (15 pts) (*) Partner 2

1. 


2. 
3. 
    a.
    b.
    c.

## Download Census zip code shapefile (10 pt)  Partner 1

1. 
```{python}
import geopandas as gdp

zip_filepath = "/Users/charismalambert/Downloads/gz_2010_us_860_00_500k.zip"
zip_file = gdp.read_file(zip_filepath)
print(zip_file.info())
```

    a. The five file types are .xml, .shx, .shp, .prj, .dbf. 
    b. The file sizes are as follows: 
    .xml = 16KB
    .shx = 837.5 MB
    .shp = 837.5 MB
    .prj = 165 bytes
    .dbf 6.4 MB
2. 
```{python}
# load zipcode shapefile
import geopandas as gdp

filepath = "/Users/charismalambert/Downloads/gz_2010_us_860_00_500k"
census_shp = gdp.read_file(filepath)

# restrict to Texas zip codes
census_shp["ZCTA5"] = census_shp["ZCTA5"].astype(str)
texas_zip = census_shp[census_shp["ZCTA5"].str.startswith(("75", "76", "77", "78", "79"))]

short_term_16["ZIP_CD"] = short_term_16["ZIP_CD"].astype(str)
hospitals_by_zip = short_term_16["ZIP_CD"].value_counts().reset_index()
hospitals_by_zip.columns = ["zip_code", "total_hospitals"]
hospitals_by_zipTX = texas_zip.merge(hospitals_by_zip, left_on = "ZCTA5", right_on = "zip_code", how = "left")

# cloropleth of hospitals by zp code in Texas
hospitals_by_zipTX = hospitals_by_zipTX.to_crs("EPSG:5070")
hospitals_by_zipTX["area_km2"] = hospitals_by_zipTX.area/1000000
hospitals_by_zipTX.plot(column = "area_km2", legend = True).set_axis_off()
```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*) Partner 2

1. 
2. 
3. 
4. 
    a.
    b.
    c.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts) Partner 1

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
